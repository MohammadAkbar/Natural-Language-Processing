{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Checker\n",
    "*by Mohammad Akbar*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check spelling we need a dictionary.<br/>\n",
    "For this program we will be using the dictionary `words.words()` from the `nltk` (natural language tool kit) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Akbar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the regex package `re`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `sortedcontainers` to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sortedcontainers import SortedSet,SortedList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `wordnet` does **NOT** include:<br/> `determiners`, `prepositions`, `pronouns`, `conjunctions`, `particles`, `auxiliary verbs`.<br/>\n",
    "Lets add these to our dictionary manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-43130b3bb31b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mgenCustom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mreadCustom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-43130b3bb31b>\u001b[0m in \u001b[0;36mgenCustom\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                     \u001b[0mACCEPTED\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./hardcode/custom_dict.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\collections.py\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;34m\"\"\"Return true if this list contains ``value``.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\collections.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;34m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0melt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\collections.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;34m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0melt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36miterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[1;31m# Get everything we can from this piece.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpiece\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_tok\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36miterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    310\u001b[0m             )\n\u001b[0;32m    311\u001b[0m             \u001b[0mnum_toks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[0mnew_filepos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m             assert new_filepos > filepos, (\n\u001b[0;32m    314\u001b[0m                 \u001b[1;34m'block reader %s() should consume at least 1 byte (filepos=%d)'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mtell\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;31m# Store our original file position, so we can return here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m         \u001b[0morig_filepos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m         \u001b[1;31m# Calculate an estimate of where we think the newline is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ACCEPTED = SortedSet([])\n",
    "notACCEPTED = SortedSet([])\n",
    "CUSTOMDICT = SortedSet([])\n",
    "ALLWORDS = SortedList([])\n",
    "ALLERRORS = SortedList([])\n",
    "\n",
    "import os\n",
    "\n",
    "def genCustom():\n",
    "    filenms = [name for name in os.listdir(\"./hardcode\") if name.endswith(\".txt\")]\n",
    "    for filenm in filenms:\n",
    "        with open(\"./hardcode/\"+filenm,'r') as file:\n",
    "            print(\"fileopened\",filenm,file)\n",
    "            for line in file:\n",
    "                print(\"*\", end =\" \")\n",
    "                word = \"\".join(line.split())\n",
    "                if word not in brown.words():\n",
    "                    ACCEPTED.add(word.lower())\n",
    "    f = open(\"./hardcode/custom_dict.txt\", \"w\")\n",
    "    for word in ACCEPTED:\n",
    "        f.write(word+\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "def readCustom():\n",
    "    with open(\"./hardcode/custom_dict.txt\",'r') as file:\n",
    "        for line in file:\n",
    "            word = \"\".join(line.split())\n",
    "            CUSTOMDICT.add(word.lower())\n",
    "\n",
    "def lookUp(word):\n",
    "    if word in CUSTOMDICT or word in brown.words():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "genCustom()\n",
    "readCustom()\n",
    "print(\"done reading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to start parsing our file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptWord(word):\n",
    "    if word not in ACCEPTED:\n",
    "        ACCEPTED.add(word)\n",
    "    if word in notACCEPTED:\n",
    "        ACCEPTED.discard(word)\n",
    "    if word in ALLERRORS:\n",
    "        ALLERRORS.discard(word)\n",
    "    ALLWORDS.add(word)\n",
    "\n",
    "def rejectWord(word):\n",
    "    if word not in notACCEPTED:\n",
    "        notACCEPTED.add(word)\n",
    "    ALLERRORS.add(word)\n",
    "\n",
    "pattern = re.compile(r\"([\\w\\-\\\\']*[a-zA-Z]+[\\w\\-\\']*)\") # regex for words with atleast 1 a-zA-Z\n",
    "with open(\"mobydick.txt\") as file:                         # open input file\n",
    "    for count , line in enumerate(file):                      # foreach line\n",
    "        for match in re.finditer(pattern, line):                 # foreach word in line\n",
    "            word = line[match.start():match.end()].lower()          # words found in line, forced lowercase\n",
    "            if word in notACCEPTED:                                 # if word already memoized\n",
    "                continue                                               # go to next word\n",
    "            elif word in ACCEPTED or lookUp(word):                  # if word in wordnet, 'asrnv' means nouns,verbs,... \n",
    "                acceptWord(word)\n",
    "            else:                                                   # else word NOT in wordnet\n",
    "                rejectWord(word)                                    # memoize as notACCEPTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have our file parsed. However, there are some false negatives in `notACCEPTED`.<br/>\n",
    "Lets account for words ending with `'s` or `s'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodApostrophe(word):\n",
    "    word_no_apst = re.sub(\"\\'s$|s\\'$\",'',word)\n",
    "    if word == word_no_apst:\n",
    "        return False\n",
    "    elif word_no_apst in ACCEPTED or lookUp(word):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in notACCEPTED:\n",
    "    if goodApostrophe(word):\n",
    "        acceptWord(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've go as far as we can with dictionaries, but there are still more words to recognize.<br/>\n",
    "Lets include compound words next `compound words` example: *gallant-cross-tree*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_compound = re.compile(r\"([^\\-\\s]+)\")\n",
    "for word in notACCEPTED:\n",
    "    accept_compound = True\n",
    "    roots = re.findall(pattern_compound, word)\n",
    "    for r , root in enumerate(roots):\n",
    "        if root in ACCEPTED or lookUp(root) or goodApostrophe(root):\n",
    "            continue\n",
    "        else:\n",
    "            accept_compound = False\n",
    "            break\n",
    "    if word.startswith('-') or word.endswith('-'):\n",
    "        accept_compound = False\n",
    "    if accept_compound:\n",
    "        acceptWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown( \"**\" \n",
    "      + format(len(ALLWORDS), ',d')\n",
    "      + \"** (*correctly spelled*) + **\"\n",
    "      + format(len(ALLERRORS), ',d')\n",
    "      + \"** (*NOT in dictionary*) = **\" \n",
    "      + format(len(ALLWORDS)+len(ALLERRORS), ',d')\n",
    "      + \"** (*total words*)<br/>**\"\n",
    "      + '{0:.2%}'.format(float(len(ALLWORDS))/float(len(ALLWORDS)+len(ALLERRORS))) \n",
    "      + \"** *correctly spelled*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import copy\n",
    "\n",
    "def insert():\n",
    "    return 1\n",
    "\n",
    "def delete():\n",
    "    return 1\n",
    "\n",
    "def replace():\n",
    "    return 1\n",
    "\n",
    "def traceBack( strA , strB , table):\n",
    "    align = []\n",
    "    linetop = \"\"\n",
    "    linemid = \"\"\n",
    "    linebot = \"\"\n",
    "    new_table = copy.deepcopy(table)\n",
    "    i = len(table)-1 \n",
    "    j = len(table[0])-1\n",
    "    while((i,j)!=(1,1)):\n",
    "        T = table[i][j]\n",
    "        S = table[i  ][j-1] + insert() if (j-1>0) else 1000000\n",
    "        D = table[i-1][j  ] + delete() if (i-1>0) else 1000000\n",
    "        R = 100000\n",
    "        if(j>1 and i>1):\n",
    "            if(table[0][j] == table[i][0]):\n",
    "                R = table[i][j]\n",
    "            else:\n",
    "                R = table[i-1][j-1] + replace()\n",
    "        #print(table[i][j],\" ? \", table[i-1][j-1])\n",
    "        if (S >= R <= D):\n",
    "            #print(\"Replace\")\n",
    "            linetop = table[i][0] + \" \" + linetop\n",
    "            linemid = \"|\" + \" \" + linemid\n",
    "            linebot = table[0][j] + \" \" + linebot\n",
    "            align = [table[i][0] + \" - \" + table[0][j]] + align\n",
    "            j = j-1\n",
    "            i = i-1\n",
    "        elif (R >= D <= S):\n",
    "            #print(\"Delete\")\n",
    "            linetop = table[i][0] + \" \"+ linetop\n",
    "            linemid = \"|\" + \" \" + linemid\n",
    "            linebot = \"*\" + \" \"+ linebot\n",
    "            align = [table[i][0] + \" - *\"] + align\n",
    "            i = i-1\n",
    "        else:\n",
    "            #print(\"Insert\")\n",
    "            linetop = \"*\" + \" \"+ linetop\n",
    "            linemid = \"|\" + \" \" + linemid\n",
    "            linebot = table[0][j] + \" \"+ linebot\n",
    "            align = [\"* - \" + table[0][j]] + align\n",
    "            j = j-1\n",
    "        new_table[i][j] = \"<b>\" + str(new_table[i][j]) + \"</b>\"\n",
    "    print('\\n'.join([linetop,linemid,linebot])) \n",
    "    #display(HTML(tabulate.tabulate(new_table, tablefmt='html')))\n",
    "        \n",
    "def minEditDist(strA , strB , max_dist):\n",
    "    m , n = len(strA) , len(strB)\n",
    "    if m==0 or n==0 :\n",
    "        return max(m,n)\n",
    "    if abs(m - n) > max_dist:\n",
    "        return abs(m - n)\n",
    "    table = [['X']*(n+2) for i in range(m+2)]\n",
    "    for i in range(m+2):\n",
    "        rowMin = max_dist + 1\n",
    "        for j in range(n+2):\n",
    "            if( (i,j)==(0,0) ):\n",
    "                table[i][j] = 'X'\n",
    "            elif( (i,j)==(0,1) or (i,j)==(1,0)):\n",
    "                table[i][j] = '#'\n",
    "            elif( i==0 ):\n",
    "                table[i][j] = strB[j-2]\n",
    "            elif( j==0 ):\n",
    "                table[i][j] = strA[i-2]\n",
    "            elif( i==1 ):\n",
    "                table[i][j] = j-1\n",
    "            elif( j==1 ):\n",
    "                table[i][j] = i-1\n",
    "            elif( strA[i-2] == strB[j-2] ):\n",
    "                table[i][j] = table[i-1][j-1]\n",
    "            else: \n",
    "                table[i][j] = min(  table[i  ][j-1] + insert(),    # Insert \n",
    "                                    table[i-1][j  ] + delete(),    # Remove \n",
    "                                    table[i-1][j-1] + replace())    # Replace\n",
    "            # check termination\n",
    "            if( i!=0 and j!=0 ):\n",
    "                rowMin = min(rowMin,table[i][j])\n",
    "        if( i!=0 and j!=0 and rowMin > max_dist ):\n",
    "            return max_dist+1\n",
    "    #if(table[i][j] < max_dist):\n",
    "        #traceBack(strA,strB,table)\n",
    "    #display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "    return table[i][j]\n",
    "minEditDist(\"intention\" , \"execution\" , 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wn.synsets('answered'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma in wn.synset('stretch.v.02').lemmas():\n",
    "    print(lemma, lemma.frame_ids())\n",
    "    print(\" | \".join(lemma.frame_strings()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "frequency_list = FreqDist(w.lower() for w in brown.words() if re.search('[a-zA-Z]+',w) )\n",
    "for word in ALLWORDS:\n",
    "    frequency_list[word.lower()] += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipypb import irange\n",
    "from operator import itemgetter, attrgetter\n",
    "from collections import deque\n",
    "print(\"here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in irange(0,len(notACCEPTED[:]),1):\n",
    "    misspelled = notACCEPTED[i]\n",
    "    minEdit = -len(misspelled)\n",
    "    top3 = deque([[\" \",minEdit,0],[\" \",minEdit,0],[\" \",minEdit,0],[\" \",minEdit,0]],3)\n",
    "    minEdit = top3[2][1]\n",
    "    for j , Dword in enumerate(frequency_list):\n",
    "        editDist = -minEditDist(misspelled , Dword , -minEdit)\n",
    "        if editDist >= minEdit:\n",
    "            f = -frequency_list[Dword]\n",
    "            entry = [Dword,editDist,f]\n",
    "            top4 = list(top3)\n",
    "            minEdit = top3[2][1]\n",
    "            top4 += [entry]\n",
    "            top4 = sorted(top4, key=itemgetter(1,2))\n",
    "            top3 = deque(top4,3)\n",
    "    top3list = list(top3)\n",
    "    top3list.reverse()\n",
    "    print(misspelled,\":\",' , '.join([row[0]+\" d=\"+str(-row[1])+\" f=\"+str(-row[2]) for row in top3list]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
