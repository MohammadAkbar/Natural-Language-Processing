{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Checker\n",
    "*by Mohammad Akbar*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check spelling we need a dictionary.<br/>\n",
    "For this program we will be using the dictionary `words.words()` from the `nltk` (natural language tool kit) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Akbar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import words as words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the regex package `re`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `sortedcontainers` to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sortedcontainers import SortedSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `wordnet` does **NOT** include:<br/> `determiners`, `prepositions`, `pronouns`, `conjunctions`, `particles`, `auxiliary verbs`.<br/>\n",
    "Lets add these to our dictionary manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'auxiliaryverbs.txt', 'conjunctions.txt', 'contractions.txt', 'determiners.txt', 'irregularverbs.txt', 'modalverbs.txt', 'particles.txt', 'prepositions.txt', 'pronouns.txt', 'staticverbs.txt']\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-359cea83c819>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./hardcode\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfilenm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./hardcode\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "ACCEPTED = SortedSet([])\n",
    "notACCEPTED = SortedSet([])\n",
    "hardfiles = [\"determiners.txt\", \n",
    "             \"prepositions.txt\", \n",
    "             \"pronouns.txt\", \n",
    "             \"conjunctions.txt\", \n",
    "             \"particles.txt\",\n",
    "             \"auxiliaryverbs.txt\",\n",
    "             \"contractions.txt\", \n",
    "             \"irregularverbs.txt\"]\n",
    "import os\n",
    "print(os.listdir(\"./hardcode\"))\n",
    "for filenm in os.listdir(\"./hardcode\"):\n",
    "    with open(filenm,'r') as file:\n",
    "        for line in file:\n",
    "            word = \"\".join(line.split())\n",
    "            if not wn.synsets(word,'asrnv'):\n",
    "                ACCEPTED.add(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to start parsing our file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"([\\w\\-\\\\']*[a-zA-Z]+[\\w\\-\\']*)\") # regex for words with atleast 1 a-zA-Z\n",
    "with open(\"mobydick.txt\") as file:                         # open input file\n",
    "    for count , line in enumerate(file):                      # foreach line\n",
    "        for match in re.finditer(pattern, line):                 # foreach word in line\n",
    "            word = line[match.start():match.end()].lower()          # words found in line, forced lowercase\n",
    "            if word in ACCEPTED or word in notACCEPTED:             # if word already memoized\n",
    "                continue                                               # go to next word\n",
    "            if wn.synsets(word,'asrnv'):                            # if word in wordnet, 'asrnv' means nouns,verbs,... \n",
    "                ACCEPTED.add(word)                                     # memoize as ACCEPTED\n",
    "            else:                                                   # if word NOT in wordnet\n",
    "                notACCEPTED.add(word)                                  # memoize as notACCEPTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have our file parsed. However, there are some false negatives in `notACCEPTED`.<br/>\n",
    "Lets account for words ending with `'s` or `s'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodApostrophe(word):\n",
    "    word_no_apst = re.sub(\"\\'s$|s\\'$\",'',word)\n",
    "    if word == word_no_apst:\n",
    "        return False\n",
    "    elif word_no_apst in ACCEPTED or wn.synsets(word_no_apst,'asrnv'):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APOSTROPHES = SortedSet([])\n",
    "for word in notACCEPTED:\n",
    "    if goodApostrophe(word):\n",
    "        APOSTROPHES.add(word)\n",
    "\n",
    "ACCEPT = ACCEPTED.union(APOSTROPHES)\n",
    "notACCEPTED = notACCEPTED.difference(APOSTROPHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown(\"**\"\n",
    "                 + format(len(APOSTROPHES), ',d')\n",
    "                 + \"** words found in dictionary, when `'s` or `s'` was removed\"\n",
    "                ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've go as far as we can with dictionaries, but there are still more words to recognize.<br/>\n",
    "Lets include compound words next `compound words` example: *gallant-cross-tree*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPOUNDWORDS = SortedSet([])\n",
    "pattern_compound = re.compile(r\"([^\\-\\s]+)\")\n",
    "for word in notACCEPTED:\n",
    "    accept_compound = True\n",
    "    roots = re.findall(pattern_compound, word)\n",
    "    for r , root in enumerate(roots):\n",
    "        if root in ACCEPTED or wn.synsets(root,'asrnv') or goodApostrophe(root):\n",
    "            continue\n",
    "        else:\n",
    "            accept_compound = False\n",
    "            break\n",
    "    if word.startswith('-') or word.endswith('-'):\n",
    "        accept_compound = False\n",
    "    if accept_compound:\n",
    "        COMPOUNDWORDS.add(word)\n",
    "print(str(len(COMPOUNDWORDS)) + \" compund words found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCEPT = ACCEPTED.union(COMPOUNDWORDS)\n",
    "notACCEPTED = notACCEPTED.difference(COMPOUNDWORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown( \"**\" \n",
    "      + format(len(ACCEPTED), ',d')\n",
    "      + \"** (*correctly spelled*) + **\"\n",
    "      + format(len(notACCEPTED), ',d')\n",
    "      + \"** (*NOT in dictionary*) = **\" \n",
    "      + format(len(ACCEPTED)+len(notACCEPTED), ',d')\n",
    "      + \"** (*total words*)<br/>**\"\n",
    "      + '{0:.2%}'.format(float(len(ACCEPTED))/float(len(ACCEPTED)+len(notACCEPTED))) \n",
    "      + \"** *correctly spelled*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets at what we have so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(str(len(notACCEPTED))+ \" words not 'yet' accepted\")\n",
    "for i , word in enumerate(notACCEPTED):\n",
    "    print(str(i+1).rjust(5) +\" \" + word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whe still need to account for tenses past,present,future and irregular verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
