{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Checker\n",
    "*by Mohammad Akbar*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check spelling we need a dictionary.<br/>\n",
    "For this program we will be using the dictionary `words.words()` from the `nltk` (natural language tool kit) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Akbar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import words as words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the regex package `re`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `sortedcontainers` to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sortedcontainers import SortedSet,SortedList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `wordnet` does **NOT** include:<br/> `determiners`, `prepositions`, `pronouns`, `conjunctions`, `particles`, `auxiliary verbs`.<br/>\n",
    "Lets add these to our dictionary manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "ACCEPTED = SortedSet([])\n",
    "notACCEPTED = SortedSet([])\n",
    "CUSTOMDICT = SortedSet([])\n",
    "ALLWORDS = []\n",
    "import os\n",
    "\n",
    "def genCustom():\n",
    "    filenms = [name for name in os.listdir(\"./hardcode\") if name.endswith(\".txt\")]\n",
    "    for filenm in filenms:\n",
    "        with open(\"./hardcode/\"+filenm,'r') as file:\n",
    "            for line in file:\n",
    "                word = \"\".join(line.split())\n",
    "                if not wn.synsets(word,'asrnv'):\n",
    "                    ACCEPTED.add(word.lower())\n",
    "    f = open(\"./hardcode/custom_dict.txt\", \"w\")\n",
    "    for word in ACCEPTED:\n",
    "        f.write(word+\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "def readCustom():\n",
    "    with open(\"./hardcode/custom_dict.txt\",'r') as file:\n",
    "        for line in file:\n",
    "            word = \"\".join(line.split())\n",
    "            CUSTOMDICT.add(word.lower())\n",
    "\n",
    "def lookUp(word):\n",
    "    if word in CUSTOMDICT or wn.synsets(word,'asrnv'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "readCustom()\n",
    "print(len(CUSTOMDICT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to start parsing our file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"([\\w\\-\\\\']*[a-zA-Z]+[\\w\\-\\']*)\") # regex for words with atleast 1 a-zA-Z\n",
    "with open(\"mobydick.txt\") as file:                         # open input file\n",
    "    for count , line in enumerate(file):                      # foreach line\n",
    "        for match in re.finditer(pattern, line):                 # foreach word in line\n",
    "            word = line[match.start():match.end()].lower()          # words found in line, forced lowercase\n",
    "            if word in ACCEPTED or word in notACCEPTED:             # if word already memoized\n",
    "                continue                                               # go to next word\n",
    "            if lookUp(word):                                        # if word in wordnet, 'asrnv' means nouns,verbs,... \n",
    "                ACCEPTED.add(word)                                     # memoize as ACCEPTED\n",
    "            else:                                                   # if word NOT in wordnet\n",
    "                notACCEPTED.add(word)                                  # memoize as notACCEPTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have our file parsed. However, there are some false negatives in `notACCEPTED`.<br/>\n",
    "Lets account for words ending with `'s` or `s'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodApostrophe(word):\n",
    "    word_no_apst = re.sub(\"\\'s$|s\\'$\",'',word)\n",
    "    if word == word_no_apst:\n",
    "        return False\n",
    "    elif word_no_apst in ACCEPTED or lookUp(word):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "APOSTROPHES = SortedSet([])\n",
    "for word in notACCEPTED:\n",
    "    if goodApostrophe(word):\n",
    "        APOSTROPHES.add(word)\n",
    "\n",
    "ACCEPT = ACCEPTED.union(APOSTROPHES)\n",
    "notACCEPTED = notACCEPTED.difference(APOSTROPHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**155** words found in dictionary, when `'s` or `s'` was removed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown(\"**\"\n",
    "                 + format(len(APOSTROPHES), ',d')\n",
    "                 + \"** words found in dictionary, when `'s` or `s'` was removed\"\n",
    "                ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've go as far as we can with dictionaries, but there are still more words to recognize.<br/>\n",
    "Lets include compound words next `compound words` example: *gallant-cross-tree*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637 compound words found!\n"
     ]
    }
   ],
   "source": [
    "COMPOUNDWORDS = SortedSet([])\n",
    "pattern_compound = re.compile(r\"([^\\-\\s]+)\")\n",
    "for word in notACCEPTED:\n",
    "    accept_compound = True\n",
    "    roots = re.findall(pattern_compound, word)\n",
    "    for r , root in enumerate(roots):\n",
    "        if root in ACCEPTED or lookUp(root) or goodApostrophe(root):\n",
    "            continue\n",
    "        else:\n",
    "            accept_compound = False\n",
    "            break\n",
    "    if word.startswith('-') or word.endswith('-'):\n",
    "        accept_compound = False\n",
    "    if accept_compound:\n",
    "        COMPOUNDWORDS.add(word)\n",
    "print(str(len(COMPOUNDWORDS)) + \" compound words found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCEPT = ACCEPTED.union(COMPOUNDWORDS)\n",
    "notACCEPTED = notACCEPTED.difference(COMPOUNDWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**11,188** (*correctly spelled*) + **1,748** (*NOT in dictionary*) = **12,936** (*total unique words*)<br/>**86.49%** *correctly spelled*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown( \"**\" \n",
    "      + format(len(ACCEPTED), ',d')\n",
    "      + \"** (*correctly spelled*) + **\"\n",
    "      + format(len(notACCEPTED), ',d')\n",
    "      + \"** (*NOT in dictionary*) = **\" \n",
    "      + format(len(ACCEPTED)+len(notACCEPTED), ',d')\n",
    "      + \"** (*total unique words*)<br/>**\"\n",
    "      + '{0:.2%}'.format(float(len(ACCEPTED))/float(len(ACCEPTED)+len(notACCEPTED))) \n",
    "      + \"** *correctly spelled*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets at what we have so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1748 words not 'yet' accepted\n"
     ]
    }
   ],
   "source": [
    "print(str(len(notACCEPTED))+ \" words not 'yet' accepted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2. ...  9. 10. 11.]\n",
      " [ 1.  0.  1. ...  8.  9. 10.]\n",
      " [ 2.  1.  0. ...  7.  8.  9.]\n",
      " ...\n",
      " [ 9.  8.  7. ...  0.  1.  2.]\n",
      " [10.  9.  8. ...  1.  0.  1.]\n",
      " [11. 10.  9. ...  2.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def closestQuerty():\n",
    "    import numpy as np\n",
    "    from tabulate import tabulate\n",
    "    quertyr = [['1','2','3','4','5','6','7','8','9','0','-'],\n",
    "               ['Q','W','E','R','T','Y','U','I','O','P',''],\n",
    "               ['A','S','D','F','G','H','J','K','L',';',''],\n",
    "               ['Z','X','C','V','B','N','M',',','.','' ,'']]\n",
    "    \n",
    "    querty = ['1','2','3','4','5','6','7','8','9','0','-'\n",
    "              ,'Q','W','E','R','T','Y','U','I','O','P'\n",
    "              ,'A','S','D','F','G','H','J','K','L',';','\\''\n",
    "              ,'Z','X','C','V','B','N','M',',','.']\n",
    "    n = len(querty)\n",
    "    adjMat = np.full((n, n), np.inf)\n",
    "    for i in range(n):\n",
    "        adjMat[i][i]=0\n",
    "    for i , row in enumerate(quertyr):\n",
    "        for j , letter in enumerate(row):\n",
    "            if letter is '':\n",
    "                continue\n",
    "            current_letter_idx = querty.index(letter)\n",
    "            unvisited = querty.copy().remove(letter)\n",
    "            visited = [letter]\n",
    "            if(i>0):\n",
    "                up_letter = quertyr[i-1][j]\n",
    "                if up_letter != '':\n",
    "                    up_letter_idx = querty.index(up_letter)\n",
    "                    adjMat[current_letter_idx][up_letter_idx] = 1\n",
    "                    adjMat[up_letter_idx][current_letter_idx] = 1\n",
    "            if(i<3):\n",
    "                up_letter = quertyr[i+1][j]\n",
    "                if up_letter != '':\n",
    "                    up_letter_idx = querty.index(up_letter)\n",
    "                    adjMat[current_letter_idx][up_letter_idx] = 1\n",
    "                    adjMat[up_letter_idx][current_letter_idx] = 1\n",
    "            if(j>0):\n",
    "                up_letter = quertyr[i][j-1]\n",
    "                if up_letter != '':\n",
    "                    up_letter_idx = querty.index(up_letter)\n",
    "                    adjMat[current_letter_idx][up_letter_idx] = 1\n",
    "                    adjMat[up_letter_idx][current_letter_idx] = 1\n",
    "            if(j<10):\n",
    "                up_letter = quertyr[i][j+1]\n",
    "                if up_letter != '':\n",
    "                    up_letter_idx = querty.index(up_letter)\n",
    "                    adjMat[current_letter_idx][up_letter_idx] = 1\n",
    "                    adjMat[up_letter_idx][current_letter_idx] = 1\n",
    "    from scipy.sparse import csr_matrix\n",
    "    from scipy.sparse.csgraph import floyd_warshall\n",
    "    dist_matrix, predecessors = floyd_warshall(csgraph=adjMat, directed=False, return_predecessors=True)\n",
    "    return dist_matrix\n",
    "quertydist = closestQuerty()\n",
    "print(quertydist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-222-95fd543b5990>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-222-95fd543b5990>\"\u001b[1;36m, line \u001b[1;32m44\u001b[0m\n\u001b[1;33m    print(freqs[alt_word.lower()]most_common()[:10])\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def getQueryQuess(c,i):\n",
    "    import numpy as np\n",
    "    querty = ['1','2','3','4','5','6','7','8','9','0','-'\n",
    "              ,'Q','W','E','R','T','Y','U','I','O','P'\n",
    "              ,'A','S','D','F','G','H','J','K','L',';','\\''\n",
    "              ,'Z','X','C','V','B','N','M',',','.']\n",
    "    index = querty.index(c)\n",
    "    idxs = np.argwhere(quertydist[index][:]==i)\n",
    "    guesses = []\n",
    "    for idx in idxs:\n",
    "        guesses.append(querty[idx[0]])\n",
    "    return guesses\n",
    "\n",
    "import operator\n",
    "nltk.download('punkt')\n",
    "words = []\n",
    "with open(\"mobydick.txt\") as file:                         # open input file\n",
    "    for line in file:\n",
    "        words += nltk.tokenize.word_tokenize(line.lower())\n",
    "\n",
    "fdist1 = nltk.FreqDist(words)\n",
    "fqdict = dict((word, freq) for word, freq in fdist1.items())\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "freqs = nltk.FreqDist(w.lower() for w in brown.words())\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "allWordExceptStopDist = nltk.FreqDist(w.lower() for w in brown.words() if w not in stopwords) \n",
    "mostCommon= allWordDist.most_common(10).keys()\n",
    "\n",
    "def closest(word):\n",
    "    print(word, end =\" : \")\n",
    "    alt_words = []\n",
    "    for i, c  in enumerate(word.upper()):\n",
    "        for j in range(15):\n",
    "            guesses = getQueryQuess(c,j)\n",
    "            for guess in guesses:\n",
    "                alt_word = word[:i] + guess + word[i+1:]\n",
    "                if lookUp(alt_word.lower()):\n",
    "                    alt_words.append(alt_word.lower())\n",
    "                    if alt_word.lower() in freqs:\n",
    "                        print(freqs[alt_word.lower()])\n",
    "                    else:\n",
    "                        print(0)\n",
    "    return alt_words\n",
    "        \n",
    "closest(\"Whas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b9ed6b917432>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'punkt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mobydick.txt\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m                         \u001b[1;31m# open input file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "import nltk\n",
    "with open(\"mobydick.txt\") as file:                         # open input file\n",
    "    for line in file:\n",
    "        words += nltk.tokenize.word_tokenize(line.lower())\n",
    "\n",
    "fdist1 = nltk.FreqDist(words)\n",
    "\n",
    "filtered_word_freq = dict((word, freq) for word, freq in fdist1.items() if word.isdigit())\n",
    "\n",
    "print(filtered_word_freq)\n",
    "for word_freq in filtered_word_freq:\n",
    "    print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
