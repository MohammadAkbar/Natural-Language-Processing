{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Get the Date<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "reddit = praw.Reddit(client_id='8G3apa5U51Rw2g',\n",
    "                     client_secret='ACPdbSJSDLV8U3EAjla2MhOj1zg',\n",
    "                     user_agent='my user agent')\n",
    "print(reddit.read_only) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Top 1000 posts from r/UpliftingNews, r/HumansBeingBros, r/NoSleep, and r/Creepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1000\n",
    "again = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if again:\n",
    "    td_neg_1 = pd.DataFrame(columns=['Text'])\n",
    "    for submission in reddit.subreddit('upliftingnews').top('year',limit=l):\n",
    "        td_neg_1 = td_neg_1.append({'Text': submission.title}, ignore_index=True)\n",
    "    td_neg_1.insert(0, 'Sentiment', 1)\n",
    "    td_neg_1.columns = [\"Sentiment\",\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if again:\n",
    "    td_neg_2 = pd.DataFrame(columns=['Text'])\n",
    "    for submission in reddit.subreddit('humansbeingbros').top('year',limit=l):\n",
    "        td_neg_2 = td_neg_2.append({'Text': submission.title}, ignore_index=True)\n",
    "    td_neg_2.insert(0, 'Sentiment', 1)\n",
    "    td_neg_2.columns = [\"Sentiment\",\"Text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if again:\n",
    "    td_pos_1 = pd.DataFrame(columns=['Text'])\n",
    "    for submission in reddit.subreddit('nosleep').top('year',limit=l):\n",
    "        td_pos_1 = td_pos_1.append({'Text': submission.title}, ignore_index=True)\n",
    "    td_pos_1.insert(0, 'Sentiment', 0)\n",
    "    td_pos_1.columns = [\"Sentiment\",\"Text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if again:\n",
    "    td_pos_2 = pd.DataFrame(columns=['Text'])\n",
    "    for submission in reddit.subreddit('creepy').top('year',limit=l):\n",
    "        td_pos_2 = td_pos_2.append({'Text': submission.title}, ignore_index=True)\n",
    "    td_pos_2.insert(0, 'Sentiment', 0)\n",
    "    td_pos_2.columns = [\"Sentiment\",\"Text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if again:\n",
    "    train_data_df = pd.concat([td_pos_1,td_pos_2,td_neg_1,td_neg_2])\n",
    "    train_data_df.to_pickle(\"./savedData.pkl\")\n",
    "    #test_data_df.to_pickle(\"./saveTest.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Clean the Data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>She Sold Happiness in Glass Jars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>If you can see this, it is very important that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>My girlfriend talks in her sleep. She's been s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Something went wrong with my heart transplant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I Answered a Spam Call</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0          0                   She Sold Happiness in Glass Jars\n",
       "1          0  If you can see this, it is very important that...\n",
       "2          0  My girlfriend talks in her sleep. She's been s...\n",
       "3          0      Something went wrong with my heart transplant\n",
       "4          0                             I Answered a Spam Call"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = pd.read_pickle(\"./savedData.pkl\")\n",
    "train_data_df.head()\n",
    "#test_data_df = pd.read_pickle(\"./saveTest.pkl\")\n",
    "#test_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove irrelevant characters, lowercase, tokenize, stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "def tokenizeMe(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", lemmatizer.lemmatize(text))\n",
    "    tokens = [t for t in nltk.word_tokenize(text) if re.search('[a-zA-Z]', t)]\n",
    "    lemma = [item for item in tokens if len(item)>2]\n",
    "    return(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert each post to a Vector of token counts<br>\n",
    "##### tri-grams<br>\n",
    "##### top 200 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range = (0,3)\n",
    "                             ,analyzer = 'word'\n",
    "                             ,tokenizer = tokenizeMe\n",
    "                             ,lowercase = True\n",
    "                             ,stop_words = 'english'\n",
    "                             ,max_features = 100)\n",
    "features = vectorizer.fit_transform(train_data_df.Text.tolist())\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Training 90% and Testing Data 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(features_nd[0:len(train_data_df)], train_data_df.Sentiment,test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Use the Data<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "X=features_nd[0:len(train_data_df)]\n",
    "y=train_data_df.Sentiment\n",
    "log_model = log_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 : Analyse the Results<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       527\n",
      "           1       0.87      0.81      0.84       549\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1076\n",
      "   macro avg       0.85      0.85      0.84      1076\n",
      "weighted avg       0.85      0.84      0.84      1076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_model.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'ago', 'away', 'baby', 'best', 'birthday', 'boy', 'bro', 'bros', 'brother', 'bus', 'cancer', 'car', 'cat', 'children', 'city', 'dad', 'daughter', 'day', 'days', 'dead', 'death', 'dog', 'dogs', 'don', 'donates', 'driver', 'family', 'food', 'free', 'friend', 'game', 'gets', 'girl', 'giving', 'going', 'good', 'got', 'guy', 'help', 'helping', 'helps', 'high', 'home', 'homeless', 'hospital', 'house', 'human', 'job', 'just', 'kid', 'kids', 'know', 'left', 'life', 'like', 'little', 'local', 'love', 'make', 'man', 'million', 'mom', 'mother', 'need', 'new', 'night', 'old', 'parents', 'people', 'plastic', 'police', 'post', 'rescue', 'room', 'save', 'saved', 'saves', 'saving', 'saw', 'school', 'son', 'state', 'stop', 'stuck', 'students', 'think', 'time', 'today', 'town', 'water', 'went', 'wife', 'woman', 'work', 'world', 'year', 'year old', 'years', 'young']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = vectorizer.get_feature_names() \n",
    "b = log_model.coef_[0]\n",
    "c = list(zip(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('think', -2.373349394634655)\n",
      "('dead', -2.2291746793884597)\n",
      "('night', -2.2271903685205863)\n",
      "('room', -2.1581187747865593)\n",
      "('death', -1.9410130718950933)\n",
      "('went', -1.6288572558335315)\n",
      "('brother', -1.205428257723142)\n",
      "('saw', -1.2021681586080415)\n",
      "('don', -1.1918456498896535)\n",
      "('house', -1.168850393458495)\n",
      "('going', -0.9920375421442347)\n",
      "('ago', -0.9748843176603189)\n",
      "('son', -0.963780545963733)\n",
      "('town', -0.9002569318613411)\n",
      "('work', -0.8699270425338524)\n",
      "('job', -0.8646266274411799)\n",
      "('friend', -0.8585100866996358)\n",
      "('know', -0.8459260353749647)\n",
      "('wife', -0.8111375745611389)\n",
      "('daughter', -0.7446471373936007)\n",
      "('parents', -0.7046059539610284)\n",
      "('like', -0.6947121623126306)\n",
      "('old', -0.6931703307227054)\n",
      "('just', -0.6562737726267913)\n",
      "('life', -0.6527945430820428)\n",
      "('left', -0.5916087236529475)\n",
      "('stuck', -0.5839670345575412)\n",
      "('got', -0.571220713439041)\n",
      "('away', -0.4281460601232218)\n",
      "('days', -0.40327242051468964)\n",
      "('human', -0.3741977597475917)\n",
      "('today', -0.36852483562980076)\n",
      "('game', -0.31037447822132036)\n",
      "('girl', -0.28224423961956235)\n",
      "('home', -0.20070521611341674)\n",
      "('new', -0.18531094770839227)\n",
      "('stop', -0.17268102266044696)\n",
      "('mother', -0.15912286582052038)\n",
      "('water', -0.1465131172763478)\n",
      "('birthday', -0.12555632774391232)\n",
      "('make', -0.028291809871630408)\n",
      "('family', -0.006462984614437874)\n",
      "('time', 0.008540384038065198)\n",
      "('years', 0.015995798617731215)\n",
      "('school', 0.060547952561472995)\n",
      "('children', 0.1402094934274058)\n",
      "('year', 0.1619803522966943)\n",
      "('post', 0.20294360933232786)\n",
      "('bus', 0.2666784025772317)\n",
      "('mom', 0.40043943373081675)\n",
      "('', 0.4061660272421985)\n",
      "('high', 0.42877620628567314)\n",
      "('city', 0.4412602548534178)\n",
      "('little', 0.4439334767199186)\n",
      "('love', 0.4585323082367906)\n",
      "('police', 0.5825117501647321)\n",
      "('day', 0.5953146466767698)\n",
      "('baby', 0.6396556659976436)\n",
      "('hospital', 0.6587754720049547)\n",
      "('people', 0.6825529716953003)\n",
      "('boy', 0.7246503105802891)\n",
      "('best', 0.8085965641900292)\n",
      "('need', 0.8128835028311099)\n",
      "('food', 0.8659857294475303)\n",
      "('world', 0.9783252179852383)\n",
      "('young', 0.9874489158726532)\n",
      "('woman', 0.9943358585170415)\n",
      "('dad', 1.004521456519194)\n",
      "('man', 1.0345189140696562)\n",
      "('kid', 1.1344170449834137)\n",
      "('local', 1.1534138646078278)\n",
      "('state', 1.1922665639631838)\n",
      "('cat', 1.2737903569524294)\n",
      "('kids', 1.28949996947516)\n",
      "('car', 1.3029433347842765)\n",
      "('good', 1.387580714889995)\n",
      "('year old', 1.3972181351835704)\n",
      "('cancer', 1.4345541454271804)\n",
      "('saved', 1.4684757373181498)\n",
      "('gets', 1.54261967936094)\n",
      "('help', 1.5596016397325543)\n",
      "('rescue', 1.5963719105707859)\n",
      "('dogs', 1.6416330854039396)\n",
      "('dog', 1.7188915466992292)\n",
      "('million', 1.7643924347571531)\n",
      "('students', 1.8196862830556926)\n",
      "('driver', 1.971787235400194)\n",
      "('plastic', 1.9946726817915386)\n",
      "('giving', 2.073764840490578)\n",
      "('free', 2.1134011307432528)\n",
      "('homeless', 2.1220236602982476)\n",
      "('save', 2.206234381731972)\n",
      "('donates', 2.265125549577727)\n",
      "('helps', 2.305641809549434)\n",
      "('guy', 2.377660015371273)\n",
      "('saves', 2.7391418130171066)\n",
      "('helping', 3.4230834936963173)\n",
      "('saving', 3.593611926807485)\n",
      "('bros', 3.9331845625090116)\n",
      "('bro', 4.20696722199846)\n"
     ]
    }
   ],
   "source": [
    "c.sort(key = lambda x: x[1]) \n",
    "print('\\n'.join(map(str,c)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
