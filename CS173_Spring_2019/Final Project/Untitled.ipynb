{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in train data: 44702\n",
      "Samples per class (train): [12500 12500 19702]\n",
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n",
      "Vocabulary size: 215947\n",
      "X_train:\n",
      "<44702x215947 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 6899505 stored elements in Compressed Sparse Row format>\n",
      "X_test: \n",
      "<25000x215947 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3645855 stored elements in Compressed Sparse Row format>\n",
      "Number of features: 215947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.62\n",
      "Best parameters:  {'C': 0.1}\n",
      "Best estimator:  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "coeffients must be 1d array or column vector, got shape (3, 215947)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-87fd24965744>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best estimator: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mmglearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize_coefficients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\mglearn\\tools.py\u001b[0m in \u001b[0;36mvisualize_coefficients\u001b[1;34m(coefficients, feature_names, n_top_features)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# this is not a row or column vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         raise ValueError(\"coeffients must be 1d array or column vector, got\"\n\u001b[1;32m---> 30\u001b[1;33m                          \" shape {}\".format(coefficients.shape))\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mcoefficients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoefficients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: coeffients must be 1d array or column vector, got shape (3, 215947)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reviews_train = load_files(\"aclImdb/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"Number of documents in train data: {}\".format(len(text_train)))\n",
    "print(\"Samples per class (train): {}\".format(np.bincount(y_train)))\n",
    "\n",
    "reviews_test = load_files(\"aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "\n",
    "\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(2, 2))\n",
    "X_train = vect.fit(text_train).transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))\n",
    "print(\"X_test: \\n{}\".format(repr(X_test)))\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)\n",
    "\n",
    "mglearn.tools.visualize_coefficients(grid.best_estimator_.coef_, feature_names, n_top_features=25)\n",
    "plt.show()\n",
    "\n",
    "lr = grid.best_estimator_\n",
    "lr.predict(X_test)\n",
    "print(\"Score: {:.2f}\".format(lr.score(X_test, y_test)))\n",
    "\n",
    "pos = [\"I've seen this story before but my kids haven't. Boy with troubled past joins military, faces his past, falls in love and becomes a man. \"\n",
    "       \"The mentor this time is played perfectly by Kevin Costner; An ordinary man with common everyday problems who lives an extraordinary \"\n",
    "       \"conviction, to save lives. After losing his team he takes a teaching position training the next generation of heroes. The young troubled \"\n",
    "       \"recruit is played by Kutcher. While his scenes with the local love interest are a tad stiff and don't generate enough heat to melt butter, \"\n",
    "       \"he compliments Costner well. I never really understood Sela Ward as the neglected wife and felt she should of wanted Costner to quit out of \"\n",
    "       \"concern for his safety as opposed to her selfish needs. But her presence on screen is a pleasure. The two unaccredited stars of this movie \"\n",
    "       \"are the Coast Guard and the Sea. Both powerful forces which should not be taken for granted in real life or this movie. The movie has some \"\n",
    "       \"slow spots and could have used the wasted 15 minutes to strengthen the character relationships. But it still works. The rescue scenes are \"\n",
    "       \"intense and well filmed and edited to provide maximum impact. This movie earns the audience applause. And the applause of my two sons.\"]\n",
    "print(\"Pos prediction: {}\". format(lr.predict(vect.transform(pos))))\n",
    "\n",
    "neg = [\"David Bryce\\'s comments nearby are exceptionally well written and informative as almost say everything \"\n",
    "       \"I feel about DARLING LILI. This massive musical is so peculiar and over blown, over produced and must have \"\n",
    "       \"caused ruptures at Paramount in 1970. It cost 22 million dollars! That is simply irresponsible. DARLING LILI \"\n",
    "       \"must have been greenlit from a board meeting that said \\\"hey we got that Pink Panther guy and that Sound Of Music gal... \"\n",
    "       \"lets get this too\\\" and handed over a blank cheque. The result is a hybrid of GIGI, ZEPPELIN, HALF A SIXPENCE, some MGM 40s \"\n",
    "       \"song and dance numbers of a style (daisies and boaters!) so hopelessly old fashioned as to be like musical porridge, and MATA HARI \"\n",
    "       \"dramatics. The production is colossal, lush, breathtaking to view, but the rest: the ridiculous romance, Julie looking befuddled, Hudson \"\n",
    "       \"already dead, the mistimed comedy, and the astoundingly boring songs deaden this spectacular film into being irritating. LILI is\"\n",
    "       \" like a twee 1940s mega musical with some vulgar bits to spice it up. STAR! released the year before sadly crashed and now is being \"\n",
    "       \"finally appreciated for the excellent film is genuinely is... and Andrews looks sublime, mature, especially in the last half hour......\"\n",
    "       \"but LILI is POPPINS and DOLLY frilly and I believe really killed off the mega musical binge of the 60s..... \"\n",
    "       \"and made Andrews look like Poppins again... which I believe was not Edwards intention. Paramount must have collectively fainted \"\n",
    "       \"when they saw this: and with another $20 million festering in CATCH 22, and $12 million in ON A CLEAR DAY and $25 million in PAINT YOUR WAGON....\"\n",
    "       \"they had a financial abyss of CLEOPATRA proportions with $77 million tied into 4 films with very uncertain futures. Maybe they should have asked seer \"\n",
    "       \"Daisy Gamble from ON A CLEAR DAY ......LILI was very popular on immediate first release in Australia and ran in 70mm cinemas for months but it failed \"\n",
    "       \"once out in the subs and the sticks and only ever surfaced after that on one night stands with ON A CLEAR DAY as a Sunday night double. Thank \"\n",
    "       \"god Paramount had their simple $1million (yes, ONE MILLION DOLLAR) film LOVE STORY and that $4 million dollar gangster pic THE GODFATHER \"\n",
    "       \"also ready to recover all the $77 million in just the next two years....for just $5m.... incredible!\"]\n",
    "print(\"Neg prediction: {}\". format(lr.predict(vect.transform(neg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
