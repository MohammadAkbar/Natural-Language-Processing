{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Get the Date<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "reddit = praw.Reddit(client_id='8G3apa5U51Rw2g',\n",
    "                     client_secret='ACPdbSJSDLV8U3EAjla2MhOj1zg',\n",
    "                     user_agent='my user agent')\n",
    "print(reddit.read_only) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Top 1000 posts from r/UpliftingNews, r/HumansBeingBros, r/NoSleep, and r/Creepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1000\n",
    "again = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if again:\n",
    "    td_neg_1 = pd.DataFrame(columns=['Text'])\n",
    "    for submission in reddit.subreddit('upliftingnews').top('year',limit=l):\n",
    "        td_neg_1 = td_neg_1.append({'Text': submission.title}, ignore_index=True)\n",
    "    td_neg_1.insert(0, 'Sentiment', 1)\n",
    "    td_neg_1.columns = [\"Sentiment\",\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if again:\n",
    "    td_neg_2 = pd.DataFrame(columns=['Text'])\n",
    "    for submission in reddit.subreddit('humansbeingbros').top('year',limit=l):\n",
    "        td_neg_2 = td_neg_2.append({'Text': submission.title}, ignore_index=True)\n",
    "    td_neg_2.insert(0, 'Sentiment', 1)\n",
    "    td_neg_2.columns = [\"Sentiment\",\"Text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if again:\n",
    "    td_pos_1 = pd.DataFrame(columns=['Text'])\n",
    "    for submission in reddit.subreddit('nosleep').top('year',limit=l):\n",
    "        td_pos_1 = td_pos_1.append({'Text': submission.title}, ignore_index=True)\n",
    "    td_pos_1.insert(0, 'Sentiment', 0)\n",
    "    td_pos_1.columns = [\"Sentiment\",\"Text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if again:\n",
    "    td_pos_2 = pd.DataFrame(columns=['Text'])\n",
    "    for submission in reddit.subreddit('creepy').top('year',limit=l):\n",
    "        td_pos_2 = td_pos_2.append({'Text': submission.title}, ignore_index=True)\n",
    "    td_pos_2.insert(0, 'Sentiment', 0)\n",
    "    td_pos_2.columns = [\"Sentiment\",\"Text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if again:\n",
    "    train_data_df = pd.concat([td_pos_1,td_pos_2,td_neg_1,td_neg_2])\n",
    "    train_data_df.to_pickle(\"./savedData.pkl\")\n",
    "    #test_data_df.to_pickle(\"./saveTest.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Clean the Data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>She Sold Happiness in Glass Jars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>If you can see this, it is very important that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>My girlfriend talks in her sleep. She's been s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Something went wrong with my heart transplant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I Answered a Spam Call</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0          0                   She Sold Happiness in Glass Jars\n",
       "1          0  If you can see this, it is very important that...\n",
       "2          0  My girlfriend talks in her sleep. She's been s...\n",
       "3          0      Something went wrong with my heart transplant\n",
       "4          0                             I Answered a Spam Call"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = pd.read_pickle(\"./savedData.pkl\")\n",
    "train_data_df.head()\n",
    "#test_data_df = pd.read_pickle(\"./saveTest.pkl\")\n",
    "#test_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove irrelevant characters, lowercase, tokenize, stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "def tokenizeMe(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", lemmatizer.lemmatize(text))\n",
    "    tokens = [t for t in nltk.word_tokenize(text) if re.search('[a-zA-Z]', t)]\n",
    "    lemma = [item for item in tokens if len(item)>2]\n",
    "    return(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert each post to a Vector of token counts<br>\n",
    "##### tri-grams<br>\n",
    "##### top 200 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range = (0,3)\n",
    "                             ,analyzer = 'word'\n",
    "                             ,tokenizer = tokenizeMe\n",
    "                             ,lowercase = True\n",
    "                             ,stop_words = 'english'\n",
    "                             ,max_features = 100)\n",
    "features = vectorizer.fit_transform(train_data_df.Text.tolist())\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Training 90% and Testing Data 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(features_nd[0:len(train_data_df)], train_data_df.Sentiment,test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Use the Data<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "X=features_nd[0:len(train_data_df)]\n",
    "y=train_data_df.Sentiment\n",
    "log_model = log_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 : Analyse the Results<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = vectorizer.get_feature_names() \n",
    "b = log_model.coef_[0]\n",
    "c = list(zip(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.sort(key = lambda x: x[1]) \n",
    "print('\\n'.join(map(str,c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
